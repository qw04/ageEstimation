# -*- coding: utf-8 -*-
"""project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Of7oU-8FyG1fdGkG16KasCIaauxe7Td
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

! pip install -q kaggle
from google.colab import files, output
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list
import kaggle
! kaggle datasets download -d frabbisw/facial-age
! mkdir ageData1
! unzip facial-age.zip -d ageData1
! kaggle datasets download -d jangedoo/utkface-new
! mkdir ageData2
! unzip utkface-new.zip -d ageData2
! kaggle datasets download -d eshachakraborty00/all-age-face-dataset
! mkdir ageData3
! unzip all-age-face-dataset.zip -d ageData3
output.clear()

import math
import numpy as np
import os
import sys
import tensorflow as tf
from PIL import Image, features
from tqdm import tqdm
from google.colab import files, output
import cv2

'''
categories should be inclusive
'''

# categories = [[[x,x+1] for x in range(1, 110, 1)]] #define both sides inclusive boundaries for labels
categories = [[1,17],
              [18,1000]]
              # [21,26],
              # [27,35],
              # [36,50],
              # [51,63],
              # [64,110]  
              # ] #manual definition 
#folder = int("")
#print(temp)

data_dir1 = "/content/ageData1/face_age"
data_dir2 = "/content/ageData2/UTKFace"
data_dir3 = "/content/ageData3/All-Age-Faces Dataset/original images"
EPOCHS = 10
IMG_WIDTH = 200
IMG_HEIGHT = 200
NUM_CATEGORIES = len(categories)
TEST_SIZE = 0.1
images = []
labels = []
names = []
file_name_counter = 0

augmentedFolder = "/content/drive/MyDrive/ComputingNea"
checkpoint_filepath = 'content/drive/MyDrive/checkpoint'

len(os.listdir(augmentedFolder))

'''
takes in data from the face age dataset
'''
# anotherPath = "/content/drive/MyDrive/someImage.JPG"

another_useless_array = []
for folder in tqdm(os.listdir(data_dir1)):
  folder_path = os.path.join(data_dir1, folder)
  if os.path.isdir(folder_path) and folder != 'face_age':
    counter = 0
    while counter < len(categories):
      if categories[counter][0] <= int(folder) <= categories[counter][1]:
        temp = counter
        break
      counter += 1
    for file in os.listdir(folder_path):
      try:
        if folder != 'face_age':
          image = Image.open(os.path.join(folder_path, file))
          for i in range(-20, 21, 5):
            newImage = image.rotate(i)
            newImage = image.resize((IMG_WIDTH, IMG_HEIGHT), resample=Image.BILINEAR)
            newTemp = f'{temp}_{i}_{file_name_counter}.jpg'
            names.append(newTemp)
            newImage.save(os.path.join(augmentedFolder, newTemp))
            file_name_counter += 1
      except Exception as e:
        print(f'{e}: {folder}')
        another_useless_array.append(folder)
        pass

another_useless_set = set(another_useless_array)
# output.clear()
print(another_useless_set)
#print(image[0])

'''
takes in data from the UTK face dataset
'''
another_useless_array = []
for randomFile in tqdm(os.listdir(data_dir2)):
  folder_path = os.path.join(data_dir2, randomFile)
  var = randomFile[:3]
  if not var.isdigit():
    var = randomFile[:2]
    if not var.isdigit():
      var = randomFile[:1]

  counter = 0 
  while counter < len(categories):
    if categories[counter][0] <= int(var) <= categories[counter][1]:
      temp = counter
      break
    counter += 1

  try:
    
    image = cv2.imread(folder_path, cv2.IMREAD_COLOR)
    for i in range(-20, 21, 5):
      file_name_counter += 1
      height, width = image.shape[:2]
      center = (width/2, height/2)
      newImage = cv2.warpAffine(src=image, M= cv2.getRotationMatrix2D(center=center, angle=i, scale=1) , dsize=(width, height))
      newImage = cv2.resize(newImage, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_LINEAR)
      newTemp = f'{temp}_{i}_{file_name_counter}.jpg'
      cv2.imwrite(os.path.join(augmentedFolder, newTemp), newImage)
  
  except Exception as e:
    another_useless_array.append(randomFile)
    pass


print(set(another_useless_array))
# output.clear()

# anotherPath = "/content/drive/MyDrive/someImage.JPG"
useless_array = []
for randomFile in tqdm(os.listdir(data_dir3)):
  folder_path = os.path.join(data_dir3, randomFile)
  counter = 0
  while counter < len(categories):
    if categories[counter][0] <= int(randomFile[-6:-4]) <= categories[counter][1]:
      temp = counter
      break
    counter += 1
  try:
    image = Image.open(folder_path)
    for i in range(-20, 21, 5):
    # for i in range(1):
      newImage = image.rotate(i)
      newImage = image.resize((IMG_WIDTH, IMG_HEIGHT), resample=Image.BILINEAR)
      newTemp = f'{temp}_{i}_{file_name_counter}.jpg'
      names.append(newTemp)
      newImage.save(os.path.join(augmentedFolder, newTemp))
      file_name_counter += 1
  except Exception as e:
    useless_array.append(randomFile)
    pass

print(set(useless_array))
output.clear()

len(names)

somePath = "/content/drive/MyDrive/wow.txt"
# a_file = open(somePath, "w")
# np.random.shuffle(names)
# a_file.write(str(names))
# a_file.close()

a_file = open(somePath, "r")
a = a_file.read().split(',')
a[len(a)-1] = a[len(a)-1][:-1]
a[0] = a[0][1:]

fileNames = a.copy()

len(fileNames)

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only use the first GPU
  
  try:
    tf.config.set_visible_devices(gpus[0], 'GPU')
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPU")
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)

def get_model():
    """
    Returns a compiled convolutional neural network model. Assume that the
    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.
    The output layer should have `NUM_CATEGORIES` units, one for each category.
    """
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(64, (3, 3), activation="sigmoid", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),
        tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),
        tf.keras.layers.Conv2D(64, (3, 3), activation="sigmoid"),
        tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 64, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 64, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 64, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 64, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 64, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES * 2, activation="sigmoid"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation="sigmoid"),
    ])
    return model

'''
after first round there is no need to run the first command
'''
model = get_model()
# model.load_weights(checkpoint_filepath)
# model.summary()

a = []
for i in range(len(labels)):
    try:
      if labels[i] == '_':
        a.append(i)
    except:
      pass
print(a)

labels = []
images = []
useless_array = []
for i in tqdm(range(60001, 75001)):
  filePath = os.path.join(augmentedFolder, fileNames[i][2:len(fileNames[i])-1])
  try:
    img = Image.open(filePath)
    images.append(np.array(img))
    label = fileNames[i][1:len(fileNames[i])-1][1]
    labels.append(label)
  except:
    output.clear()
    useless_array.append(fileNames[i][2:len(fileNames[i])-1])
print(len(useless_array))

labels = tf.keras.utils.to_categorical(labels)
x_train = np.array(images[:len(images) - math.floor(len(images)*TEST_SIZE)])
y_train = np.array(labels[:len(images) - math.floor(len(images)*TEST_SIZE)])
x_test = np.array(images[len(images) - math.floor(len(images)*TEST_SIZE)+1:])
y_test = np.array(labels[len(images) - math.floor(len(images)*TEST_SIZE)+1:])

fileNames[1]

del labels.tolist()[1]
labels = np.array(labels)
# fileNames[0]

model.compile(
        optimizer="adam",
        loss="BinaryCrossentropy",
        metrics=["BinaryCrossentropy"],
    )

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_binary_crossentropy',
    mode='max',
    save_best_only=True)

model.fit(x_train,
          y_train,
          epochs = EPOCHS,
          validation_data=(x_test, y_test),
          callbacks=[model_checkpoint_callback])

'''
finding out the number of images for different age ranges
'''
useless_array = labels.copy()
useless_set = set(useless_array)
useless_dit = []

for i in useless_set:
  useless_dit.append([i, useless_array.count(i)])

useless_dit = sorted(useless_dit ,key=lambda x: (x[0]))
x = list(map(lambda x:[int(x[0]),x[1]], useless_dit))
for i in x:
  print(f'{categories[i[0]][0]}-{categories[i[0]][1]} : {i[1]}')

temp = [images, labels]
temp = np.array(temp, dtype=object).T
np.random.shuffle(temp)
temp = temp.T
temp = temp.tolist()
images = temp[0]
labels = temp[1]
labels = tf.keras.utils.to_categorical(labels)
imageBuckets = [images[i:i+500] for i in range(0, len(images), 500)]
labelBuckets = [labels[i:i+500] for i in range(0, len(images), 500)]

totalBuckets = [x for x in range(len(imageBuckets))]
np.random.shuffle(totalBuckets)
variable = math.ceil(len(totalBuckets) * TEST_SIZE)
trainingBuckets = totalBuckets[:variable]
testingBuckets = totalBuckets[variable+1:]

np.array(imageBuckets[0]).shape

for e in range(EPOCHS):
  np.random.shuffle(trainingBuckets)
  for i in trainingBuckets:
    model.fit(np.array(imageBuckets[i]), np.array(labelBuckets[i]))
  output.clear()

np.random.shuffle(testingBuckets)
for i in testingBuckets:
    model.evaluate(np.array(imageBuckets[i]),  np.array(labelBuckets[i]), verbose=2)

for i in trainingBuckets:
  print(set(labelBuckets[i]))

model.save('/content/drive/MyDrive/Colab Notebooks/model')